import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
# # -*- coding: utf-8 -*-
# """BookRecomendationsystemBasedOnRevew.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1dzfi-_OZNlDcOSJcSHlUEp-GK4AY2FqW
# """

# # !pip install flask flask-ngrok

# # !pip install --upgrade huggingface_hub transformers

# # !pip install nltk

# # pip install transformers[torch]

# # !pip install spacy  pandas scikit-learn



# """# Import dataset from google drive"""

# # from google.colab import drive
# # drive.mount('/content/drive/')

# """## check datasets

# ### Books
# """
# import os


# # Ensure the CSV path is correct
# csv_path = os.path.join(os.path.dirname(__file__), "25000_books.csv")  # ✅ Fix path
# print(f"Looking for CSV at: {csv_path}")  # Debugging print

# df_Books = pd.read_csv(csv_path)  # ✅ Load CSV from Backend folder




# #df_Books = pd.read_csv("C:/Users/shari/Documents/Book recommender system/Backend/25000_books.csv")

# #df_Books = pd.read_csv('/content/drive/MyDrive/25000_books.csv')
# print(df_Books.shape)

# df_Books.shape

# df_Books.head()

# """# Preparing dataset"""

# # Missing Value Count Function
# def show_missing():
#     missing = df_Books.columns[df_Books.isnull().any()].tolist()
#     return missing

# # Missing data counts and percentage
# print('Missing Data Count')
# print(df_Books[show_missing()].isnull().sum().sort_values(ascending = False))
# print('--'*50)
# print('Missing Data Percentage')
# print(round(df_Books[show_missing()].isnull().sum().sort_values(ascending = False)/len(df_Books)*100,2))

# #Dropping Nulls of Books data as they are negligible
# df_Books.dropna(inplace=True)

# df_Books.dropna(subset=['Title'], inplace=True)

# df_Books = df_Books[df_Books['Title'] != '']

# df_Books['publishedYear'].unique()

# #converting Reviews type into integer
# df_Books['publishedYear'] = pd.to_numeric(df_Books['publishedYear'])
# df_Books['publishedYear'].unique()

# #Capping the outlier rows with Percentiles
# upper_lim = df_Books['publishedYear'].quantile(.95)
# lower_lim = df_Books['publishedYear'].quantile(.05)
# df_Books.loc[(df_Books["publishedYear"] > upper_lim),"publishedYear"] = upper_lim
# df_Books.loc[(df_Books["publishedYear"] < lower_lim),"publishedYear"] = lower_lim

# df_Books['publishedYear'].unique()

# # Define the clean_text function
# def clean_text(text):
#     # Remove HTML tags
#     text = re.sub(r'<.*?>', '', text)
#     # Remove special characters
#     text = re.sub(r'[^A-Za-z\s]', '', text)
#     # Convert to lowercase and strip whitespace
#     text = text.lower().strip()
#     return text

# # Define a function to clean and split categories
# def clean_and_split_categories(text):
#     if isinstance(text, str):  # Ensure the input is a string
#         # Split the categories by comma
#         categories = text.split(',')
#         # Clean each category using clean_text function
#         cleaned_categories = [clean_text(category) for category in categories]
#         return cleaned_categories
#     elif isinstance(text, list):  # If the input is already a list, clean each item
#         return [clean_text(category) for category in text]
#     else:
#         return []

# #df_Books['cleaned_review'] = df_Books['review/text'].apply(clean_text)
# df_Books['Title'] = df_Books['Title'].apply(clean_text)
# df_Books['authors'] = df_Books['authors'].apply(clean_text)

# # Apply the clean_and_split_categories function to the 'categories' column
# df_Books['categories'] = df_Books['categories'].apply(clean_and_split_categories)

# df_Books.head(3)

# df_Books['categories']

# df_Books.shape

# """# Browsing on dataset"""

# #barplot of age and its counts
# plt.figure(figsize=(20,7))
# sns.countplot(x = df_Books['publishedYear'])
# plt.title('Book year Distribution',fontsize=15)
# plt.grid()
# plt.show()

# # Top 10 sold Books
# Top10_Book = df_Books['Title'].value_counts().reset_index().head(10)
# Top10_Book.columns = ['Title', 'count']
# Top10_Book

# # Bar plot of Top 10 sold Books
# plt.rcParams['figure.figsize'] = (10, 5)
# sns.barplot(x='Title', y='count', data=Top10_Book)
# plt.xticks(rotation=70, horizontalalignment="center")
# plt.grid()
# plt.title('Top 10 Sold Books')
# plt.show()

# #Top 10 author with most books written
# Top10_author=df_Books['authors'].value_counts().reset_index().head(10)
# Top10_author.columns = ['authors', 'count']
# Top10_author

# #barplot of top 10 Authors with most books written
# sns.barplot(x="authors",y="count",data=Top10_author)
# plt.xticks(rotation=70, horizontalalignment="center")
# plt.title("Barplot of top 10 Authors",fontsize=20)

# #Top 10 publisher
# Top10_publisher=df_Books['publisher'].value_counts().reset_index().head(10)
# Top10_publisher.columns = ['publisher', 'count']
# Top10_publisher

# #barplot of top 10 publisher
# sns.barplot(x="publisher",y="count",data=Top10_publisher)
# plt.xticks(rotation=70, horizontalalignment="center")
# plt.title("Barplot of top 10 publisher",fontsize=20)

# """# **Start**: recommendation system

## Import libraries
"""

import spacy

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import nltk
import re
import matplotlib.pyplot as plt
import nltk
import spacy

# %matplotlib inline

import torch
# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""## Remove the book which does not have 10 reviews
#Since our dataset is large, and we conduct review analysis, we believe requiring at least 10 reviews per book would be beneficial in identifying public opinion.
"""

# Group by 'Title' and filter books with at least 10 reviews
df_Books = df_Books.groupby('Title').filter(lambda x: len(x) >= 10)

df_Books.shape

df_Books.head(1)

df = df_Books.groupby('Title').agg({
    'review/score': 'mean',
    'review/text': lambda x: ' . '.join(x),
    'authors': 'first',
    'publisher': 'first',
    'categories': 'first',
    'publishedYear': 'first'
}).reset_index()

# Rename columns for clarity
df = df.rename(columns={
    'review/score': 'average_score',
    'review/text': 'merged_reviews'
})

df = df[df['Title'] != '']

df_Books = df_Books[df_Books['Title'].isin(df['Title'])]

df.shape

df_Books.shape

df.head(5)

"""## Data Preprocessing"""

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text)  # Remove extra whitespace
    words = text.split()
    words = [word for word in words if word not in stop_words]
    words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(words)

df['cleaned_reviews'] = df['merged_reviews'].apply(preprocess_text)

"""## Entity Recognition"""

# !python -m spacy download en_core_web_sm

"""**Utilize spaCy to extract named entities, and fine-tune the NER model to improve accuracy on your specific dataset.**"""

import spacy
nlp = spacy.load('en_core_web_sm')

from tqdm import tqdm

# Function to recognize entities
def recognize_entities(text):
    doc = nlp(text)
    entities = [(entity.text, entity.label_) for entity in doc.ents]
    return entities

df['entities'] = df['merged_reviews'].apply(recognize_entities)

df.head(5)

from collections import Counter

def get_common_entities(df, title, entity_type=None):
    book_reviews = df[df['Title'] == title]
    entities = [entity for sublist in book_reviews['entities'] for entity in sublist]
    if entity_type:
        entities = [entity for entity in entities if entity[1] == entity_type]
    return Counter(entities).most_common()

common_entities = get_common_entities(df, 'a clockwork orange')
print("Common Entities:", common_entities)

"""## Sentiment Analysis"""

# ! pip install textblob

import nltk
nltk.download('punkt')
from textblob import TextBlob
import pandas as pd

def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity, blob.sentiment.subjectivity

def analyze_book_reviews(df, df_Books):
    # Initialize new columns in df
    df['avg_sentiment'] = 0.0
    df['min_review'] = ''
    df['max_review'] = ''
    df['sentiment_polarity'] = 0.0
    df['sentiment_subjectivity'] = 0.0
    df['positive_feedback'] = [[] for _ in range(len(df))]
    df['negative_feedback'] = [[] for _ in range(len(df))]

    for index, row in df.iterrows():
        title = row['Title']

        # Find all reviews for this book in df_Books
        book_reviews = df_Books[df_Books['Title'] == title]['review/text'].tolist()

        if book_reviews:
            # Calculate sentiments for all reviews of this book
            sentiments = [analyze_sentiment(review) for review in book_reviews]
            polarities = [s[0] for s in sentiments]
            subjectivities = [s[1] for s in sentiments]

            # Calculate average sentiment
            avg_sentiment = sum(polarities) / len(polarities)
            avg_subjectivity = sum(subjectivities) / len(subjectivities)

            # Find reviews with minimum and maximum sentiment
            min_sentiment = min(polarities)
            max_sentiment = max(polarities)

            min_review = book_reviews[polarities.index(min_sentiment)]
            max_review = book_reviews[polarities.index(max_sentiment)]

            positive_feedback = []
            negative_feedback = []

            for polarity, review in zip(polarities, book_reviews):
                if polarity > 0.1:  # Positive
                    positive_feedback.append(review)
                elif polarity < -0.1:  # Negative
                    negative_feedback.append(review)

            # Update the DataFrame
            df.at[index, 'avg_sentiment'] = avg_sentiment
            df.at[index, 'min_review'] = min_review
            df.at[index, 'max_review'] = max_review
            df.at[index, 'sentiment_polarity'] = avg_sentiment
            df.at[index, 'sentiment_subjectivity'] = avg_subjectivity
            df.at[index, 'positive_feedback'] = positive_feedback
            df.at[index, 'negative_feedback'] = negative_feedback
        else:
            # If no reviews found, set default values
            df.at[index, 'avg_sentiment'] = None
            df.at[index, 'min_review'] = "No reviews found"
            df.at[index, 'max_review'] = "No reviews found"
            df.at[index, 'sentiment_polarity'] = None
            df.at[index, 'sentiment_subjectivity'] = None
            df.at[index, 'positive_feedback'] = None
            df.at[index, 'negative_feedback'] = None

    return df

df = analyze_book_reviews(df, df_Books)

df.head(5)

"""## Knowledge Graph Integration

## Knowledge Graph
"""

# !pip install networkx

"""load graph object from file"""

#import networkx as nx
#G= nx.read_graphml("graph.graphml")
#print(G.edges())

import networkx as nx
import matplotlib.pyplot as plt

G = nx.MultiDiGraph()

# Add nodes and edges from the DataFrame
for index, row in df.iterrows():
    G.add_node(row['Title'], type='Book', publishedYear=row['publishedYear'], sentiment=row['avg_sentiment'])
    G.add_node(row['authors'], type='Author')
    G.add_node(row['publisher'], type='Publisher')
    for category in row['categories']:
        G.add_node(category, type='Category')
        G.add_edge(row['Title'], category, relation='BELONGS_TO')
    G.add_edge(row['Title'], row['authors'], relation='WRITTEN_BY')
    G.add_edge(row['Title'], row['publisher'], relation='PUBLISHED_BY')
    for entity in row['entities']:
        G.add_node(entity[0], type=entity[1])
        G.add_edge(row['Title'], entity[0], relation='MENTIONED_IN')


# # Visualize the updated knowledge graph
# plt.figure(figsize=(15, 15))
# pos = nx.spring_layout(G, k=0.5)
# nx.draw(G, pos, with_labels=True, node_size=50, font_size=10)
# plt.show()

"""Store graph object to file"""

#nx.write_graphml(G, "graph.graphml")

def create_first_row_graph(df):
    G = nx.MultiDiGraph()

    # Get the first row
    row = df.iloc[0]

    # Add book node
    G.add_node(row['Title'], type='Book', publishedYear=row['publishedYear'], sentiment=row['avg_sentiment'])

    # Add author node and edge
    G.add_node(row['authors'], type='Author')
    G.add_edge(row['Title'], row['authors'], relation='WRITTEN_BY')

    # Add publisher node and edge
    G.add_node(row['publisher'], type='Publisher')
    G.add_edge(row['Title'], row['publisher'], relation='PUBLISHED_BY')

    # Add category nodes and edges
    for category in row['categories']:
        G.add_node(category, type='Category')
        G.add_edge(row['Title'], category, relation='BELONGS_TO')

    # Add entity nodes and edges
    for entity in row['entities']:
        G.add_node(entity[0], type=entity[1])
        G.add_edge(row['Title'], entity[0], relation='MENTIONED_IN')


    return G

# def visualize_graph(G):
#     plt.figure(figsize=(15, 15))
#     pos = nx.spring_layout(G, k=0.5, iterations=50)

#     # Draw nodes
#     nx.draw_networkx_nodes(G, pos, node_size=3000, node_color='lightblue')
#     nx.draw_networkx_labels(G, pos, font_size=8, font_weight="bold")

#     # Draw edges
#     nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True)

#     # Add edge labels
#     edge_labels = nx.get_edge_attributes(G, 'relation')
#     nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)

#     plt.title("Knowledge Graph for First Book", fontsize=16)
#     plt.axis('off')
#     plt.tight_layout()
#     plt.show()

# # Usage
# Gf = create_first_row_graph(df)
# visualize_graph(Gf)

"""## Prepare Data for LDA:"""

# !pip install gensim nltk

"""Upload preprocessed dataset and prepare dataset"""

#df = pd.read_csv('/content/df.csv')
#print(df.shape)
import nltk
nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import gensim
from gensim import corpora
from gensim.models import CoherenceModel

# Clean text
def clean_text(text):
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'[^A-Za-z\s]', '', text)  # Remove special characters
    text = text.lower().strip()  # Convert to lowercase and strip whitespace
    return text

df['cleaned_reviews'] = df['merged_reviews'].apply(clean_text)

# Tokenize and remove stopwords
stop_words = set(stopwords.words('english'))

def preprocess(text):
    tokens = word_tokenize(text)
    return [word for word in tokens if word not in stop_words]

df['tokenized_review'] = df['cleaned_reviews'].apply(preprocess)

# Create dictionary and corpus
dictionary = corpora.Dictionary(df['tokenized_review'])
corpus = [dictionary.doc2bow(text) for text in df['tokenized_review']]

from gensim import corpora, models
from gensim.models.coherencemodel import CoherenceModel


# Train the final LDA model
lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=7, passes=15, alpha='auto', eta='auto', random_state=42)

# Print the topics
topics = lda_model.print_topics(num_words=5)
print(topics)

# Evaluate topic coherence
coherence_model_lda = CoherenceModel(model=lda_model, texts=df['tokenized_review'], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

df.head(5)

# !pip install pyLDAvis

import pyLDAvis.gensim

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=dictionary)
pyLDAvis.display(vis)

# Assign topics to each review
df['topics'] = df['tokenized_review'].apply(lambda x: sorted(lda_model.get_document_topics(dictionary.doc2bow(x)), key=lambda tup: -tup[1])[0][0])

# Group by book title to find the most common topics for each book
book_topics = df.groupby('Title')['topics'].apply(lambda x: x.value_counts().index[0]).reset_index()

book_topics

"""## Recommendation System"""

def summarization(text):
    # Join text if it's a list
    if isinstance(text, list):
        text = ' '.join(text)

    # Tokenize the input text
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

    # Generate summary IDs
    summary_ids = model.generate(inputs, max_length=100, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)

    # Decode the generated summary IDs
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    return summary
from transformers import BartTokenizer, BartForConditionalGeneration

"""## Feedback Generation

Using a knowledge graph to grain general information about the book.
"""

def bookInformation(title):
    if title not in G:
        return f"No feedback available for '{title}'."

    feedback = f"Feedback for '{title}':\n"

    # Overall sentiment
    sentiment = df.loc[df['Title'] == title, 'avg_sentiment'].values[0]
    feedback += f"Overall sentiment: {'Positive' if sentiment > 0 else 'Negative' if sentiment < 0 else 'Neutral'}\n"

    # Book relationships
    category = [n for n in G.neighbors(title) if G.nodes[n]['type'] == 'Category']
    if category:
        feedback += f"This book belongs to the '{category[0]}' category.\n"

    author = [n for n in G.neighbors(title) if G.nodes[n]['type'] == 'Author']
    if author:
        feedback += f"The book is written by '{author[0]}'.\n"

    publisher = [n for n in G.neighbors(title) if G.nodes[n]['type'] == 'Publisher']
    if publisher:
        feedback += f"The book is published by '{publisher[0]}'.\n"

    return feedback

"""### using DistilBERT for summarization"""

model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

def summarization(text):
    # Join text if it's a list
    if isinstance(text, list):
        text = ' '.join(text)

    # Tokenize the input text
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

    # Generate summary IDs
    summary_ids = model.generate(inputs, max_length=100, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)

    # Decode the generated summary IDs
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    return summary

def generate_positive_feedback(title):
    if title not in G:
        return f"No feedback available for '{title}'."

    feedback = f"Feedback for '{title}':\n"

    positive_feedback = summarization(df.loc[df['Title'] == title, 'positive_feedback'].values[0])

    # Lowest and highest sentiment sentences
    highest_sentiment = df.loc[df['Title'] == title, 'max_review'].values[0]

    feedback += f"The Summary of positive review: \"{str(positive_feedback)}\"\n"
    feedback += "\n"
    feedback += f"The Most positive review: \"{highest_sentiment}\"\n"

    return feedback

def generate_negative_feedback(title):
    if title not in G:
        return f"No feedback available for '{title}'."

    feedback = f"Feedback for '{title}':\n"

    negative_feedback = summarization(df.loc[df['Title'] == title, 'negative_feedback'].values[0])

    # Lowest and highest sentiment sentences
    lowest_sentiment = df.loc[df['Title'] == title, 'min_review'].values[0]
    feedback += f"The Summary of negative review: \"{str(negative_feedback)}\"\n"
    feedback += "\n"
    feedback += f"The Most negative review: \"{lowest_sentiment}\"\n"

    return feedback

"""### Using Entities"""

def get_recommendations_with_entities(title, df, top_n=5, entity_type=None):
    common_entities = get_common_entities(df, title, entity_type)
    entity_set = set([entity[0] for entity in common_entities])

    def count_common_entities(entities):
        return len(entity_set.intersection(set([entity[0] for entity in entities])))

    df['common_entity_count'] = df['entities'].apply(count_common_entities)
    recommendations = df[df['Title'] != title].sort_values(by='common_entity_count', ascending=False)
    recommended_titles = recommendations['Title'].unique()[:top_n]
    return recommended_titles

"""### Using Topics"""

from collections import defaultdict
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from gensim import corpora, models
from transformers import pipeline
import re
import nltk

def extract_representative_sentences(df, dictionary, lda_model, num_sentences=3):
    topic_sentences = defaultdict(list)

    for _, row in df.iterrows():
        bow = dictionary.doc2bow(row['tokenized_review'])
        topics = lda_model.get_document_topics(bow)
        if topics:
            dominant_topic = sorted(topics, key=lambda tup: -tup[1])[0][0]
            sentences = sent_tokenize(row['merged_reviews'])
            topic_sentences[dominant_topic].extend(sentences[:num_sentences])

    return topic_sentences

# Extract representative sentences for each topic
topic_sentences = extract_representative_sentences(df, dictionary, lda_model)

# Load the summarization pipeline using BART
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Function to truncate text if it exceeds the max length
def truncate_text(text, max_length=1024):
    tokens = tokenizer(text, truncation=True, max_length=max_length, return_tensors="pt")
    return tokenizer.decode(tokens.input_ids[0], skip_special_tokens=True)

# Provide concise feedback using summarization
def provide_concise_feedback(book_title, book_topics, topic_sentences, lda_model, summarizer, num_sentences=3):
    # Check if the book title exists in book_topics
    if book_title not in book_topics['Title'].values:
        return f"No topics found for the book titled '{book_title}'."

    main_topic = book_topics[book_topics['Title'] == book_title]['topics'].values[0]
    topic_words = lda_model.show_topic(main_topic, topn=7)

    feedback = f"The main topics for '{book_title}' include: {[word for word, _ in topic_words]}\n\n"
    feedback += "Key points from reviews:\n"

    # Check if there are any sentences to summarize
    if main_topic not in topic_sentences or not topic_sentences[main_topic]:
        return f"No representative sentences found for the main topic of '{book_title}'."

    sentences_to_summarize = " ".join(topic_sentences[main_topic])
    sentences_to_summarize = truncate_text(sentences_to_summarize, max_length=1024)

    summarized = summarizer(sentences_to_summarize, max_length=num_sentences*20, min_length=num_sentences*5, do_sample=False)

    feedback += summarized[0]['summary_text']

    return feedback

"""## use"""

sample_title = df['Title'].iloc[0]

"""### book information
With the below function, we can gain general information about a book.
"""

print(bookInformation(sample_title))

"""### Recomendation based on reviews

#### Positive Reviews
We are looking for positive public opinion about the book based on the reviews.


*   Showing summary of all reviews with the help of the DistilBERT library.
*   Showing the most positive review about the book.
"""

print(generate_positive_feedback(sample_title))

"""#### Negative Reviews
We are looking for negative public opinion about the book based on the reviews.
*   Showing summary of all reviews with the help of the DistilBERT library.
*   Showing the most Negative review about the book.
"""

print(generate_negative_feedback(sample_title))

"""### Reviews similarity
With the help of the below function, we use entities and find books with similar entities.
"""

recommended_books = get_recommendations_with_entities(sample_title, df)
print("Recommended Books with Similar Entities:", recommended_books)

"""### Get feedback based on the review text's main topics.
Using LDA and the DistilBERT library, we can automatically summarize review text. By identifying the main topics, we can then create a concise sentence that captures the key information behind the review.
"""

feedback = provide_concise_feedback(sample_title, book_topics, topic_sentences, lda_model, summarizer)
print(feedback)

"""# Flask"""

from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok

app = Flask(__name__)
run_with_ngrok(app)

# !ngrok authtoken "ngrok authontication code"

"""Just wen need ngrok url run the below code
when you had a url, it will get a error (in free mode you can just have one url)
"""

import subprocess

# Start ngrok in the background
subprocess.Popen(['ngrok', 'http', '5000'])

import requests
import time

# Allow ngrok some time to start
time.sleep(5)

# Fetch the ngrok URL
ngrok_url = requests.get('http://localhost:4040/api/tunnels').json()['tunnels'][0]['public_url']
print('Ngrok URL:', ngrok_url)

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/webhook', methods=['POST'])
def webhook():
    req = request.get_json(silent=True, force=True)
    print("Request:", req)

    intent_name = req['queryResult']['intent']['displayName']
    parameters = req['queryResult']['parameters']
    sample_title = parameters.get('booktitle')

    if sample_title:
        if isinstance(sample_title, list):
            sample_title = sample_title[0]
        elif not isinstance(sample_title, str):
            pass

        if intent_name == 'General Information':
            response_text = bookInformation(sample_title)
            #response_text = 'General Information'
        elif intent_name == 'PositiveFeedback':
            response_text = generate_positive_feedback(sample_title)
           #response_text = 'PositiveFeedback'
        elif intent_name == 'NegativeFeedback':
            response_text = generate_negative_feedback(sample_title)
            #response_text = 'NegativeFeedback'
        elif intent_name == 'SimilarBooksEntites':
            #response_text = get_recommendations_with_entities(sample_title, df)
            response_text = 'SimilarBooksEntites'
        elif intent_name == 'ReviewTopics':
            response_text = provide_concise_feedback(sample_title, book_topics, topic_sentences, lda_model, summarizer)
            #response_text = 'ReviewTopics'
        else:
            response_text = "Unknown intent."
    else:
        response_text = "sample_title parameter is missing."

    res = {
        "fulfillmentText": response_text
    }

    return jsonify(res)

if __name__ == '__main__':
    app.run(port=5000)